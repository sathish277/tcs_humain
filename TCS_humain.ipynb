{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing the neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading the one sample of train data...for actual i need to read all the data but for sample code i took the sample data of 50000 rows of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"E:/TCS_data/train.csv\",nrows=50000)\n",
    "#test=pd.read_csv(\"E:/TCS_data/Test.csv\",nrows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.columns) # getting the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.head()\n",
    "#print(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicate rows\n",
    "train=train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove html tags from data\n",
    "def remove_html(text):\n",
    "    tag = re.compile(r'<.*?>')\n",
    "    return tag.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the html tags from Body column\n",
    "for i in range(train.shape[0]):\n",
    "    train.loc[i, 'Body'] = remove_html(train.loc[i,'Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_words(sentence):#cleaning the text data\n",
    "    sentence = sentence.lower()                # Converting to lowercase\n",
    "    rem=re.compile(r'[^\\w]')\n",
    "    sentence=re.sub(rem,r' ', sentence)\n",
    "    sentence = re.sub(r'[?$|!|\\':|\"|#]',r' ',sentence)\n",
    "    sentence = re.sub(r'[.|,|)|(|\\|/]\\\"\\\"]',r' ',sentence)#Removing Punctuations\n",
    "    sentence=re.sub(r'\\d+',' ',sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing  unneccessary symbols from\n",
    "for column in train:\n",
    "    if(column=='Title' or column=='Body'):\n",
    "        for i in range(train.shape[0]):\n",
    "            train.loc[i, column] =clean_words(train.loc[i,column])\n",
    "# for column in train:\n",
    "#     index=0\n",
    "#     if(column=='Title' or column=='Body'):\n",
    "#         for sentence in train[column]:\n",
    "#             cleaned_word=clean_words(sentence)\n",
    "#             train[column][index]=cleaned_word  \n",
    "#             index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words \n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tokenized_doc_body=train['Body'].apply(lambda x:x.split())\n",
    "tokenized_doc_title=train['Title'].apply(lambda x:x.split())\n",
    "#remove stop_words\n",
    "tokenized_doc_body=tokenized_doc_body.apply(lambda x:[item for item in x if item not in stop_words])\n",
    "tokenized_doc_title=tokenized_doc_title.apply(lambda x:[item for item in x if item not in stop_words])\n",
    "#de-tokenization\n",
    "detokenized_doc_body=[]\n",
    "detokenized_doc_title=[]\n",
    "for i in range(train.shape[0]):\n",
    "    t1=' '.join(tokenized_doc_body[i])\n",
    "    t2=' '.join(tokenized_doc_title[i])\n",
    "    detokenized_doc_body.append(t1)\n",
    "    detokenized_doc_title.append(t2)\n",
    "train['Body']= detokenized_doc_body\n",
    "train['Title']= detokenized_doc_title\n",
    "# def stop_words_remove(text):\n",
    "#     tokenize_text=text.apply(lambda x:x.split())\n",
    "#     final=tokenize_text.apply(lambda x:[item for item in x if item not in stop_words])\n",
    "#     #de-tokenize\n",
    "#     de_token=' '.join(final)\n",
    "#     return de_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(7)\n",
    "# for column in train:\n",
    "#     if(column=='Title' or column=='Body'):\n",
    "#         for i in range(train.shape[0]):\n",
    "#             train.loc[i, column] =stop_words_remove(train.loc[i,column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoding the tags using multilabelbinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode tags to multi-hot\n",
    "train_tags=[]\n",
    "for tag in train['Tags'].values:\n",
    "    tags=[i for i in tag.split()]\n",
    "    train_tags.append(tags)\n",
    "#print(train_topics,'\\n')\n",
    "topic_encoder=MultiLabelBinarizer()\n",
    "topic_encoded=topic_encoder.fit_transform(train_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tags=len(topic_encoded[0])\n",
    "print(num_tags)\n",
    "print(topic_encoder.classes_)\n",
    "print(topic_encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to get document term matrix we use this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "class TextPreprocessor(object):\n",
    "    def __init__(self,vocab_size):\n",
    "        self._vocab_size=vocab_size\n",
    "        self._tokenizer=None\n",
    "    def create_tokenizer(self,text_list):\n",
    "        tokenizer=text.Tokenizer(num_words=self._vocab_size)\n",
    "        tokenizer.fit_on_texts(text_list)\n",
    "        self._tokenizer=tokenizer\n",
    "    def transform_text(self,text_list):\n",
    "        text_matrix=self._tokenizer.texts_to_matrix(text_list)\n",
    "        return text_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_body=train['Body'].values\n",
    "train_title=train['Title'].values\n",
    "# #test_text=test_data['Review Text'].values\n",
    "# #test_title=test_data['Review Title'].values\n",
    "# for i in range(train.shape[0]):\n",
    "#     train[i]=train_text[i]+train_title[i]\n",
    "# for i in range(test_text.shape[0]):\n",
    "#     test_text[i]=test_text[i]+test_title[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=500\n",
    "processor=TextPreprocessor(vocab_size)\n",
    "processor.create_tokenizer(train_body)\n",
    "body_text=processor.transform_text(train_body)\n",
    "body_title=processor.transform_text(train_tile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# horizontally stacking up the  body and title column term matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data=np.hstack(body_tile,body_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(body_text[0]))\n",
    "print(body_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(body_text.shape) #  final size of the term matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model\n",
    "def create_model(num_topics,vocab_size):\n",
    "    model=Sequential()\n",
    "    model.add(Dense(128,input_shape=(vocab_size,),activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(num_topics,activation='softmax'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model=create_model(num_tags,vocab_size)\n",
    "model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final model\n",
    "model.fit(total_data,topic_encoded,epochs=10,batch_size=64,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now our model is ready for the predictions.Let's test this model using test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"E:/TCS_data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head() #top 5 rown of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape # shape of the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train.drop_duplicates() # drop_duplicates of test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(test.shape[0]):#remove html tags\n",
    "    test.loc[i, 'Body'] = remove_html(test.loc[i,'Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in test: # remove everything else which is scrap in the test dataset\n",
    "    if(column=='Title' or column=='Body'):\n",
    "        for i in range(test.shape[0]):\n",
    "            test.loc[i, column] =clean_words(test.loc[i,column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tokenized_doc_body_test=test['Body'].apply(lambda x:x.split())\n",
    "tokenized_doc_title_test=test['Title'].apply(lambda x:x.split())\n",
    "#remove stop_words\n",
    "tokenized_doc_body_test=tokenized_doc_body_test.apply(lambda x:[item for item in x if item not in stop_words])\n",
    "tokenized_doc_title_test=tokenized_doc_title_test.apply(lambda x:[item for item in x if item not in stop_words])\n",
    "#de-tokenization\n",
    "detokenized_doc_body_test=[]\n",
    "detokenized_doc_title_test=[]\n",
    "for i in range(test.shape[0]):\n",
    "    t1=' '.join(tokenized_doc_body_test[i])\n",
    "    t2=' '.join(tokenized_doc_title_test[i])\n",
    "    detokenized_doc_body_test.append(t1)\n",
    "    detokenized_doc_title_test.append(t2)\n",
    "test['Body']= detokenized_doc_body_test\n",
    "test['Title']= detokenized_doc_title_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_body=test['Body'].values\n",
    "test_title=test['Title'].values#getting the values of the columns of body and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the term matrix for the test data\n",
    "vocab_size=500\n",
    "processor=TextPreprocessor(vocab_size)\n",
    "processor.create_tokenizer(test_body)\n",
    "body_text_test=processor.transform_text(test_body)\n",
    "body_title_test=processor.transform_text(test_tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_test=np.hstack(body_title_test,body_text_test)#horizontal stacking up of test body and title columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finally we are at predictions step,the predicted_labels contains the tags for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels=model.predict(total_data_test,batch_size=64)#predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this is my idea for the given problem statement using NLP and deep learning.My next task is to optimize the neural network for better accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
